{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = False\n",
    "b = False\n",
    "a | b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6691,  0.2654,  0.3564, -0.8418, -0.2603],\n",
      "         [-0.9283,  0.0884, -0.0517, -0.2380, -0.6756],\n",
      "         [-0.3518,  0.2873, -0.2232, -0.9393, -0.2547]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.word2idx[word] for word in text.split()]\n",
    "\n",
    "    def detokenize(self, tokens):\n",
    "        return ' '.join([self.idx2word[token] for token in tokens])\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, V), attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        x, attn = self.attention(query, key, value, mask)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.layer_norms[0](x + self.self_attn(x, x, x, mask))\n",
    "        return self.layer_norms[1](x + self.feed_forward(x))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        x = self.layer_norms[0](x + self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.layer_norms[1](x + self.cross_attn(x, memory, memory, memory_mask))\n",
    "        return self.layer_norms[2](x + self.feed_forward(x))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        x = self.positional_enc(self.token_emb(src))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        x = self.positional_enc(self.token_emb(tgt))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "        return x\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = nn.Linear(encoder.layers[0].self_attn.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
    "        return self.generator(output)\n",
    "\n",
    "# Example usage\n",
    "vocab = ['<pad>', '<sos>', '<eos>', 'hello', 'world']\n",
    "tokenizer = Tokenizer(vocab)\n",
    "vocab_size = len(vocab)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "max_len = 10\n",
    "\n",
    "encoder = Encoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_len)\n",
    "decoder = Decoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_len)\n",
    "model = Seq2Seq(encoder, decoder, vocab_size)\n",
    "\n",
    "src_sentence = \"hello world\"\n",
    "tgt_sentence = \"<sos> world hello <eos>\"\n",
    "\n",
    "src_tokens = tokenizer.tokenize(src_sentence)\n",
    "tgt_tokens = tokenizer.tokenize(tgt_sentence)\n",
    "\n",
    "src_tensor = torch.tensor(src_tokens).unsqueeze(0)\n",
    "tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0)\n",
    "\n",
    "output = model(src_tensor, tgt_tensor[:, :-1])\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
